{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOyHhUS1LeapL/kC8M7kdoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishika-Pattnaik/Reviews-sentiment-analysis/blob/main/ML_Task02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dow6GVYGTcEa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis"
      ],
      "metadata": {
        "id": "EcWhJfszTrhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Preprocessing text"
      ],
      "metadata": {
        "id": "dpDmNaklTusk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleaning text"
      ],
      "metadata": {
        "id": "F29dfaqfT9RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "dataset_path = '/content/sample_data/aclImdb_v1.tar'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with tarfile.open(dataset_path, 'r') as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"‚úÖ Dataset extracted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KKKBLWcSwd4",
        "outputId": "0a64ea7d-6e04-4d87-df52-d514bbf13089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1963353769.py:7: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from pathlib import Path\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download required NLTK data (one-time setup)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "class SentimentAwarePreprocessorTFIDF:\n",
        "    \"\"\"\n",
        "    Text preprocessor optimized for TF-IDF + Logistic Regression.\n",
        "    Focuses on noise reduction and vocabulary simplification while preserving sentiment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize lemmatizer\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # Standard English stopwords\n",
        "        self.base_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "        # Preserve sentiment-important words\n",
        "        self.sentiment_preservers = {\n",
        "            'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing', 'nowhere',\n",
        "            'without', 'barely', 'hardly', 'scarcely', 'seldom', 'rarely',\n",
        "            'very', 'really', 'extremely', 'quite', 'rather', 'too', 'so', 'such',\n",
        "            'more', 'most', 'much', 'many', 'few', 'little', 'less', 'least',\n",
        "            'but', 'however', 'although', 'though', 'despite', 'yet',\n",
        "            'good', 'bad', 'best', 'worst', 'better', 'worse'\n",
        "        }\n",
        "\n",
        "        # Remove sentiment preservers from stopwords\n",
        "        self.stopwords = self.base_stopwords - self.sentiment_preservers\n",
        "\n",
        "        # Simplified emoticon handling (convert to basic tokens)\n",
        "        self.emoticon_pattern = r'[:\\-;=][\\)\\(\\[\\]DPpOo\\|\\\\\\/\\{\\}@><\\*]|[\\)\\(\\[\\]DPpOo\\|\\\\\\/\\{\\}@><\\*][:\\-;=]'\n",
        "\n",
        "        # Simplified emphasis pattern (normalize to single character)\n",
        "        self.emphasis_pattern = r'([a-zA-Z])\\1{2,}'\n",
        "\n",
        "    def clean_text_efficiently(self, text):\n",
        "        \"\"\"\n",
        "        Clean text for TF-IDF: remove noise, preserve sentiment, simplify features.\n",
        "        \"\"\"\n",
        "        if pd.isna(text) or text == '':\n",
        "            return ''\n",
        "\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Handle emoticons (simplify to POSITIVE/NEGATIVE)\n",
        "        emoticons = re.findall(self.emoticon_pattern, text)\n",
        "        for i, emoticon in enumerate(emoticons):\n",
        "            text = text.replace(emoticon, f'EMOTICON{i}', 1)\n",
        "\n",
        "        # Remove URLs, HTML tags, and emails\n",
        "        text = re.sub(r'https?://[^\\s]+', '', text)\n",
        "        text = re.sub(r'www\\.[^\\s]+', '', text)\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Handle contractions\n",
        "        contraction_fixes = {\n",
        "            \"won't\": \"will not\", \"can't\": \"can not\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
        "            \"'m\": \" am\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "            \"wasn't\": \"was not\", \"weren't\": \"were not\", \"isn't\": \"is not\",\n",
        "            \"aren't\": \"are not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "            \"wouldn't\": \"would not\", \"shouldn't\": \"should not\", \"couldn't\": \"could not\"\n",
        "        }\n",
        "        for contraction, expansion in contraction_fixes.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "\n",
        "        # Remove all punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Normalize emphasis (e.g., \"sooo\" -> \"so\")\n",
        "        text = re.sub(self.emphasis_pattern, r'\\1', text)\n",
        "\n",
        "        # Restore emoticons as simple tokens\n",
        "        for i, emoticon in enumerate(emoticons):\n",
        "            text = text.replace(f'EMOTICON{i}', 'POSITIVE' if emoticon in [':)', ':-)', ':D', '=)'] else 'NEGATIVE')\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_filter(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize and filter for TF-IDF: lemmatize and remove stopwords.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Remove short tokens (except sentiment preservers)\n",
        "        tokens = [token for token in tokens if len(token) >= 2 or token in self.sentiment_preservers]\n",
        "\n",
        "        # Remove stopwords (except sentiment preservers)\n",
        "        tokens = [token for token in tokens if token not in self.stopwords]\n",
        "\n",
        "        # Lemmatize\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline for a single text.\n",
        "        \"\"\"\n",
        "        cleaned_text = self.clean_text_efficiently(text)\n",
        "        tokens = self.tokenize_and_filter(cleaned_text)\n",
        "        return ' '.join(tokens) if tokens else 'EMPTY_REVIEW'\n",
        "\n",
        "    def preprocess_corpus(self, texts):\n",
        "        \"\"\"\n",
        "        Preprocess entire corpus efficiently.\n",
        "        \"\"\"\n",
        "        return [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "def load_imdb_dataset(dataset_path):\n",
        "    \"\"\"\n",
        "    Load IMDb dataset from folder structure.\n",
        "    \"\"\"\n",
        "    print(\"üìÅ Loading IMDb Movie Reviews Dataset\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    dataset_path = Path(\"/content/aclImdb\")\n",
        "\n",
        "    if not dataset_path.exists():\n",
        "        print(f\"‚ùå Dataset path '{dataset_path}' not found!\")\n",
        "        return None\n",
        "\n",
        "    train_path = dataset_path / \"train\"\n",
        "    test_path = dataset_path / \"test\"\n",
        "\n",
        "    if not train_path.exists() or not test_path.exists():\n",
        "        print(\"‚ùå Train or test folders not found!\")\n",
        "        return None\n",
        "\n",
        "    def load_reviews_from_folder(folder_path, label, split_name):\n",
        "        reviews, labels, ratings = [], [], []\n",
        "        for sentiment, sent_label in [('pos', 1), ('neg', 0)]:\n",
        "            if label not in [sentiment, 'both']:\n",
        "                continue\n",
        "            sent_path = folder_path / sentiment\n",
        "            if not sent_path.exists():\n",
        "                print(f\"‚ö†Ô∏è Warning: {sent_path} not found. Skipping {sentiment} reviews.\")\n",
        "                continue\n",
        "            files = list(sent_path.glob(\"*.txt\"))\n",
        "            if not files:\n",
        "                print(f\"‚ö†Ô∏è Warning: No .txt files found in {sent_path}.\")\n",
        "                continue\n",
        "            for file_path in files:\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        review = f.read().strip()\n",
        "                        if not review:\n",
        "                            print(f\"‚ö†Ô∏è Warning: Empty file {file_path}\")\n",
        "                            continue\n",
        "                        reviews.append(review)\n",
        "                        labels.append(sent_label)\n",
        "                        rating = int(file_path.stem.split('_')[1])\n",
        "                        ratings.append(rating)\n",
        "                except (UnicodeDecodeError, ValueError) as e:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Failed to process {file_path}: {e}\")\n",
        "        return reviews, labels, ratings\n",
        "\n",
        "    train_reviews, train_labels, train_ratings = load_reviews_from_folder(train_path, 'both', 'train')\n",
        "    test_reviews, test_labels, test_ratings = load_reviews_from_folder(test_path, 'both', 'test')\n",
        "\n",
        "    train_df = pd.DataFrame({\n",
        "        'review': train_reviews,\n",
        "        'sentiment': train_labels,\n",
        "        'rating': train_ratings,\n",
        "        'split': 'train'\n",
        "    })\n",
        "\n",
        "    test_df = pd.DataFrame({\n",
        "        'review': test_reviews,\n",
        "        'sentiment': test_labels,\n",
        "        'rating': test_ratings,\n",
        "        'split': 'test'\n",
        "    })\n",
        "\n",
        "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"Total reviews: {len(df):,}\")\n",
        "    print(f\"Training reviews: {len(train_df):,}\")\n",
        "    print(f\"Test reviews: {len(test_df):,}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess_for_tfidf(df, preprocessor, output_file=\"imdb_tfidf.csv\"):\n",
        "    \"\"\"\n",
        "    Apply preprocessing and save data for TF-IDF + Logistic Regression.\n",
        "    \"\"\"\n",
        "    print(\"\\nüöÄ Applying TF-IDF Preprocessing\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Preprocess reviews\n",
        "    df['processed_review'] = preprocessor.preprocess_corpus(df['review'].tolist())\n",
        "\n",
        "    # Handle empty reviews\n",
        "    empty_reviews = df['processed_review'].str.strip() == 'EMPTY_REVIEW'\n",
        "    print(f\"‚ö†Ô∏è Empty reviews after preprocessing: {empty_reviews.sum()}\")\n",
        "    df = df[~empty_reviews].copy()\n",
        "\n",
        "    # Save preprocessed data\n",
        "    model_data = df[['processed_review', 'sentiment', 'split']].copy()\n",
        "    model_data.to_csv(output_file, index=False)\n",
        "    print(f\"‚úÖ Preprocessed data saved to {output_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def vectorize_tfidf(df, max_features=5000):\n",
        "    \"\"\"\n",
        "    Convert preprocessed text to TF-IDF vectors.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Converting to TF-IDF Vectors\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "    X_train = vectorizer.fit_transform(df[df['split'] == 'train']['processed_review'])\n",
        "    X_test = vectorizer.transform(df[df['split'] == 'test']['processed_review'])\n",
        "    y_train = df[df['split'] == 'train']['sentiment']\n",
        "    y_test = df[df['split'] == 'test']['sentiment']\n",
        "\n",
        "    print(f\"‚úÖ TF-IDF vectors created:\")\n",
        "    print(f\"  Training shape: {X_train.shape}\")\n",
        "    print(f\"  Test shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, vectorizer\n",
        "\n",
        "def main_tfidf_workflow(dataset_path):\n",
        "    \"\"\"\n",
        "    Preprocessing workflow for TF-IDF + Logistic Regression, without visualizations.\n",
        "    \"\"\"\n",
        "    print(\"üßπ Preprocessing for TF-IDF + Logistic Regression\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load dataset\n",
        "    df = load_imdb_dataset(dataset_path)\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = SentimentAwarePreprocessorTFIDF()\n",
        "    print(\"‚úÖ Preprocessor initialized for TF-IDF\")\n",
        "\n",
        "    # Preprocess data\n",
        "    df = preprocess_for_tfidf(df, preprocessor)\n",
        "\n",
        "    # Vectorize data\n",
        "    X_train, X_test, y_train, y_test, vectorizer = vectorize_tfidf(df)\n",
        "\n",
        "    print(\"\\nüéâ Preprocessing Completed!\")\n",
        "    print(\"‚úÖ Data is ready for Logistic Regression training\")\n",
        "    print(f\"  Saved preprocessed data to: imdb_tfidf.csv\")\n",
        "    print(f\"  X_train shape: {X_train.shape}\")\n",
        "    print(f\"  X_test shape: {X_test.shape}\")\n",
        "\n",
        "    return df, X_train, X_test, y_train, y_test, vectorizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # UPDATE THIS PATH TO YOUR DATASET FOLDER\n",
        "    dataset_path = \"path/to/your/aclImdb\"  # Change this!\n",
        "\n",
        "    df, X_train, X_test, y_train, y_test, vectorizer = main_tfidf_workflow(dataset_path)\n",
        "\n",
        "    # You can now train Logistic Regression using X_train, y_train\n",
        "    # Example:\n",
        "    # from sklearn.linear_model import LogisticRegression\n",
        "    # model = LogisticRegression(max_iter=1000)\n",
        "    # model.fit(X_train, y_train)\n",
        "    # y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "8uUaf6pfUBl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0a9054-336e-4596-fb44-078374a3435f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Preprocessing for TF-IDF + Logistic Regression\n",
            "==================================================\n",
            "üìÅ Loading IMDb Movie Reviews Dataset\n",
            "==================================================\n",
            "‚úÖ Dataset loaded successfully!\n",
            "Total reviews: 50,000\n",
            "Training reviews: 25,000\n",
            "Test reviews: 25,000\n",
            "‚úÖ Preprocessor initialized for TF-IDF\n",
            "\n",
            "üöÄ Applying TF-IDF Preprocessing\n",
            "==================================================\n",
            "‚ö†Ô∏è Empty reviews after preprocessing: 0\n",
            "‚úÖ Preprocessed data saved to imdb_tfidf.csv\n",
            "\n",
            "üìä Converting to TF-IDF Vectors\n",
            "==================================================\n",
            "‚úÖ TF-IDF vectors created:\n",
            "  Training shape: (25000, 5000)\n",
            "  Test shape: (25000, 5000)\n",
            "\n",
            "üéâ Preprocessing Completed!\n",
            "‚úÖ Data is ready for Logistic Regression training\n",
            "  Saved preprocessed data to: imdb_tfidf.csv\n",
            "  X_train shape: (25000, 5000)\n",
            "  X_test shape: (25000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training TF_IDF + Logistic Regression model\n",
        "\n"
      ],
      "metadata": {
        "id": "lU9UKK6FbWH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "lr_model=LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred= lr_model.predict(X_test)\n",
        "\n",
        "accuracy_lr= accuracy_score(y_test, y_pred)\n",
        "precision_lr=precision_score(y_test, y_pred, average=\"weighted\")\n",
        "f1_lr=f1_score(y_test, y_pred, average=\"weighted\")\n",
        "recall_lr=recall_score(y_test, y_pred, average=\"weighted\")\n",
        "print(f\"Accuracy= {accuracy_lr: .4f}\")\n",
        "print(f\"Precision= {precision_lr: .4f}\")\n",
        "print(f\"F1 Score= {f1_lr: .4f}\")\n",
        "print(f\"Recall= {recall_lr: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yn85h7MZBVN",
        "outputId": "b5118c0f-6373-4a9b-b105-3bc61d5b82b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy=  0.8833\n",
            "Precision=  0.8834\n",
            "F1 Score=  0.8833\n",
            "Recall=  0.8833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model"
      ],
      "metadata": {
        "id": "OSdV6r-vbznP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(lr_model, 'logistic_regression_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvuTVR2MbysO",
        "outputId": "4581bfcb-740d-4b24-dcda-84ab0e0dee8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM model"
      ],
      "metadata": {
        "id": "6fT6t_JAbcqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing data"
      ],
      "metadata": {
        "id": "lSjKtXExddYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from pathlib import Path\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "class SentimentAwarePreprocessorLSTM:\n",
        "    \"\"\"\n",
        "    Text preprocessor optimized for LSTM: preserves emoticons, emphasis, and sentiment.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.base_stopwords = set(stopwords.words('english'))\n",
        "        self.sentiment_preservers = {\n",
        "            'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing', 'nowhere',\n",
        "            'without', 'barely', 'hardly', 'scarcely', 'seldom', 'rarely',\n",
        "            'very', 'really', 'extremely', 'quite', 'rather', 'too', 'so', 'such',\n",
        "            'more', 'most', 'much', 'many', 'few', 'little', 'less', 'least',\n",
        "            'but', 'however', 'although', 'though', 'despite', 'yet',\n",
        "            'good', 'bad', 'best', 'worst', 'better', 'worse'\n",
        "        }\n",
        "        self.stopwords = self.base_stopwords - self.sentiment_preservers\n",
        "        self.emoticon_pattern = r'[:\\-;=][\\)\\(\\[\\]DPpOo\\|\\\\\\/\\{\\}@><\\*]|[\\)\\(\\[\\]DPpOo\\|\\\\\\/\\{\\}@><\\*][:\\-;=]'\n",
        "        self.emphasis_pattern = r'([a-zA-Z])\\1{2,}'\n",
        "\n",
        "    def clean_text_efficiently(self, text):\n",
        "        \"\"\"\n",
        "        Clean text for LSTM: moderate cleaning, preserve emoticons and emphasis.\n",
        "        \"\"\"\n",
        "        if pd.isna(text) or text == '':\n",
        "            return ''\n",
        "        text = str(text).lower()\n",
        "        # Preserve emoticons as tokens\n",
        "        emoticons = re.findall(self.emoticon_pattern, text)\n",
        "        for i, emoticon in enumerate(emoticons):\n",
        "            text = text.replace(emoticon, f' EMOTICON_{\"POSITIVE\" if emoticon in [\":)\", \":-)\", \":D\", \"=)\"] else \"NEGATIVE\"} ', 1)\n",
        "        # Preserve emphasis\n",
        "        text = re.sub(self.emphasis_pattern, r'\\1 EMPHASIS', text)\n",
        "        # Remove URLs, HTML tags, emails\n",
        "        text = re.sub(r'https?://[^\\s]+', '', text)\n",
        "        text = re.sub(r'www\\.[^\\s]+', '', text)\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        # Handle contractions\n",
        "        contraction_fixes = {\n",
        "            \"won't\": \"will not\", \"can't\": \"can not\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
        "            \"'m\": \" am\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "            \"wasn't\": \"was not\", \"weren't\": \"were not\", \"isn't\": \"is not\",\n",
        "            \"aren't\": \"are not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "            \"wouldn't\": \"would not\", \"shouldn't\": \"should not\", \"couldn't\": \"could not\"\n",
        "        }\n",
        "        for contraction, expansion in contraction_fixes.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "        # Keep punctuation for context\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_filter(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize and filter: lemmatize, keep minimal stopwords.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "        tokens = text.split()\n",
        "        tokens = [token for token in tokens if len(token) >= 2 or token in self.sentiment_preservers or token.startswith('EMOTICON_') or token == 'EMPHASIS']\n",
        "        tokens = [self.lemmatizer.lemmatize(token) if not (token.startswith('EMOTICON_') or token == 'EMPHASIS') else token for token in tokens]\n",
        "        return tokens\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline for a single text.\n",
        "        \"\"\"\n",
        "        cleaned_text = self.clean_text_efficiently(text)\n",
        "        tokens = self.tokenize_and_filter(cleaned_text)\n",
        "        return ' '.join(tokens) if tokens else 'EMPTY_REVIEW'\n",
        "\n",
        "    def preprocess_corpus(self, texts):\n",
        "        \"\"\"\n",
        "        Preprocess entire corpus.\n",
        "        \"\"\"\n",
        "        return [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "def load_imdb_dataset(dataset_path):\n",
        "    \"\"\"\n",
        "    Load IMDb dataset from folder structure.\n",
        "    \"\"\"\n",
        "    print(\"üìÅ Loading IMDb Dataset\")\n",
        "    dataset_path = Path(dataset_path)\n",
        "    if not dataset_path.exists():\n",
        "        print(f\"‚ùå Path '{dataset_path}' not found!\")\n",
        "        return None\n",
        "    train_path = dataset_path / \"train\"\n",
        "    test_path = dataset_path / \"test\"\n",
        "    if not train_path.exists() or not test_path.exists():\n",
        "        print(\"‚ùå Train/test folders not found!\")\n",
        "        return None\n",
        "\n",
        "    def load_reviews_from_folder(folder_path, label, split_name):\n",
        "        reviews, labels, ratings = [], [], []\n",
        "        for sentiment, sent_label in [('pos', 1), ('neg', 0)]:\n",
        "            if label not in [sentiment, 'both']:\n",
        "                continue\n",
        "            sent_path = folder_path / sentiment\n",
        "            if not sent_path.exists():\n",
        "                print(f\"‚ö†Ô∏è {sent_path} not found. Skipping.\")\n",
        "                continue\n",
        "            files = list(sent_path.glob(\"*.txt\"))\n",
        "            if not files:\n",
        "                print(f\"‚ö†Ô∏è No .txt files in {sent_path}.\")\n",
        "                continue\n",
        "            for file_path in files:\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        review = f.read().strip()\n",
        "                        if not review:\n",
        "                            print(f\"‚ö†Ô∏è Empty file {file_path}\")\n",
        "                            continue\n",
        "                        reviews.append(review)\n",
        "                        labels.append(sent_label)\n",
        "                        rating = int(file_path.stem.split('_')[1])\n",
        "                        ratings.append(rating)\n",
        "                except (UnicodeDecodeError, ValueError) as e:\n",
        "                    print(f\"‚ö†Ô∏è Failed to process {file_path}: {e}\")\n",
        "        return reviews, labels, ratings\n",
        "\n",
        "    train_reviews, train_labels, train_ratings = load_reviews_from_folder(train_path, 'both', 'train')\n",
        "    test_reviews, test_labels, test_ratings = load_reviews_from_folder(test_path, 'both', 'test')\n",
        "    train_df = pd.DataFrame({'review': train_reviews, 'sentiment': train_labels, 'rating': train_ratings, 'split': 'train'})\n",
        "    test_df = pd.DataFrame({'review': test_reviews, 'sentiment': test_labels, 'rating': test_ratings, 'split': 'test'})\n",
        "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "    print(f\"‚úÖ Loaded {len(df):,} reviews (Train: {len(train_df):,}, Test: {len(test_df):,})\")\n",
        "    return df\n",
        "\n",
        "def preprocess_for_lstm(df, preprocessor, output_file=\"imdb_lstm.csv\", max_words=10000, max_len=200):\n",
        "    \"\"\"\n",
        "    Preprocess and tokenize for LSTM, save data.\n",
        "    \"\"\"\n",
        "    print(\"\\nüöÄ Applying LSTM Preprocessing\")\n",
        "    df['processed_review'] = preprocessor.preprocess_corpus(df['review'].tolist())\n",
        "    empty_reviews = df['processed_review'].str.strip() == 'EMPTY_REVIEW'\n",
        "    print(f\"‚ö†Ô∏è Empty reviews: {empty_reviews.sum()}\")\n",
        "    df = df[~empty_reviews].copy()\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(df[df['split'] == 'train']['processed_review'])\n",
        "    X_train = pad_sequences(tokenizer.texts_to_sequences(df[df['split'] == 'train']['processed_review']), maxlen=max_len)\n",
        "    X_test = pad_sequences(tokenizer.texts_to_sequences(df[df['split'] == 'test']['processed_review']), maxlen=max_len)\n",
        "    y_train = df[df['split'] == 'train']['sentiment'].values\n",
        "    y_test = df[df['split'] == 'test']['sentiment'].values\n",
        "    model_data = df[['processed_review', 'sentiment', 'split']].copy()\n",
        "    model_data.to_csv(output_file, index=False)\n",
        "    print(f\"‚úÖ Saved to {output_file}\")\n",
        "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "    return df, X_train, X_test, y_train, y_test, tokenizer\n",
        "\n",
        "def main_lstm_workflow(dataset_path):\n",
        "    \"\"\"\n",
        "    Preprocessing workflow for LSTM.\n",
        "    \"\"\"\n",
        "    print(\"üßπ Preprocessing for LSTM\")\n",
        "    df = load_imdb_dataset(dataset_path)\n",
        "    if df is None:\n",
        "        return None\n",
        "    preprocessor = SentimentAwarePreprocessorLSTM()\n",
        "    df, X_train, X_test, y_train, y_test, tokenizer = preprocess_for_lstm(df, preprocessor)\n",
        "    print(\"\\nüéâ Preprocessing Completed!\")\n",
        "    print(\"‚úÖ Data ready for LSTM training\")\n",
        "    return df, X_train, X_test, y_train, y_test, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/aclImdb\"  # Update if needed\n",
        "    df, X_train, X_test, y_train, y_test, tokenizer = main_lstm_workflow(dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ZcMSgtbnE-",
        "outputId": "6707d716-8056-4c0d-d8c9-7b44b54ee019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Preprocessing for LSTM\n",
            "üìÅ Loading IMDb Dataset\n",
            "‚úÖ Loaded 50,000 reviews (Train: 25,000, Test: 25,000)\n",
            "\n",
            "üöÄ Applying LSTM Preprocessing\n",
            "‚ö†Ô∏è Empty reviews: 0\n",
            "‚úÖ Saved to imdb_lstm.csv\n",
            "X_train shape: (25000, 200), X_test shape: (25000, 200)\n",
            "\n",
            "üéâ Preprocessing Completed!\n",
            "‚úÖ Data ready for LSTM training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training model"
      ],
      "metadata": {
        "id": "Aq4lsquddfxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=200),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDYRgMyHdUsy",
        "outputId": "e169ae9a-4fd4-4733-d34b-d3f0e95cbbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7323 - loss: 0.5137 - val_accuracy: 0.8562 - val_loss: 0.3402\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.8959 - loss: 0.2665 - val_accuracy: 0.8638 - val_loss: 0.3202\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.9325 - loss: 0.1825 - val_accuracy: 0.8616 - val_loss: 0.3594\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9568 - loss: 0.1245 - val_accuracy: 0.8487 - val_loss: 0.3772\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.9679 - loss: 0.0925 - val_accuracy: 0.8490 - val_loss: 0.4422\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9790 - loss: 0.0638 - val_accuracy: 0.8561 - val_loss: 0.4822\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.9769 - loss: 0.0704 - val_accuracy: 0.8528 - val_loss: 0.5859\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.9871 - loss: 0.0399 - val_accuracy: 0.8502 - val_loss: 0.6484\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 0.9896 - loss: 0.0310 - val_accuracy: 0.8476 - val_loss: 0.5584\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.9861 - loss: 0.0418 - val_accuracy: 0.8535 - val_loss: 0.5922\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9966 - loss: 0.0137 - val_accuracy: 0.8320 - val_loss: 0.8927\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9963 - loss: 0.0149 - val_accuracy: 0.8539 - val_loss: 0.8089\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9925 - loss: 0.0256 - val_accuracy: 0.8495 - val_loss: 0.8157\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9976 - loss: 0.0081 - val_accuracy: 0.8396 - val_loss: 0.8549\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9970 - loss: 0.0126 - val_accuracy: 0.8485 - val_loss: 0.8497\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9978 - loss: 0.0113 - val_accuracy: 0.8544 - val_loss: 0.8385\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.8507 - val_loss: 0.7176\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.9956 - loss: 0.0151 - val_accuracy: 0.8524 - val_loss: 0.8340\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 0.8442 - val_loss: 0.7971\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9991 - loss: 0.0047 - val_accuracy: 0.8358 - val_loss: 0.9365\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 0.9993 - loss: 0.0040 - val_accuracy: 0.8524 - val_loss: 0.8854\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.8510 - val_loss: 0.9357\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9971 - loss: 0.0088 - val_accuracy: 0.8427 - val_loss: 0.9256\n",
            "Epoch 24/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.9982 - loss: 0.0069 - val_accuracy: 0.8516 - val_loss: 0.9694\n",
            "Epoch 25/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9971 - loss: 0.0094 - val_accuracy: 0.8544 - val_loss: 1.0117\n",
            "Epoch 26/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.8281e-04 - val_accuracy: 0.8539 - val_loss: 1.0747\n",
            "Epoch 27/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.8159e-05 - val_accuracy: 0.8544 - val_loss: 1.1531\n",
            "Epoch 28/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 4.0519e-05 - val_accuracy: 0.8546 - val_loss: 1.2058\n",
            "Epoch 29/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.5124e-05 - val_accuracy: 0.8545 - val_loss: 1.2529\n",
            "Epoch 30/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 1.4926e-05 - val_accuracy: 0.8547 - val_loss: 1.2994\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8495 - loss: 1.3320\n",
            "Test Accuracy: 0.8547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old snippet.Optimizing the model further."
      ],
      "metadata": {
        "id": "YjL39nmrthgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# 1Ô∏è‚É£ Fine-tune with a lower learning rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),  # smaller LR for fine-tuning\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 2Ô∏è‚É£ Callbacks to maximize benefit in few epochs\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=1, verbose=1\n",
        ")  # reduces LR if model stops improving\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', patience=2, restore_best_weights=True, verbose=1\n",
        ")  # stops early if no improvement\n",
        "\n",
        "# 3Ô∏è‚É£ Continue training for only 5 epochs\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr, early_stop]\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ Evaluate again\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Optimized Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6oDG2FMtk-v",
        "outputId": "c5945848-bf0e-4243-ea67-55459f93d1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 7.6909e-06 - val_accuracy: 0.8540 - val_loss: 1.4509 - learning_rate: 1.0000e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m779/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.2222e-06\n",
            "Epoch 2: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 3.2206e-06 - val_accuracy: 0.8538 - val_loss: 1.5411 - learning_rate: 1.0000e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m778/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.6961e-06\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.6960e-06 - val_accuracy: 0.8538 - val_loss: 1.5774 - learning_rate: 5.0000e-05\n",
            "Epoch 3: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 1.4980\n",
            "Optimized Test Accuracy: 0.8540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEW!!! Optimised LSTM using GloVe"
      ],
      "metadata": {
        "id": "Ixqb7pkDEjLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEtODbjZCn6l",
        "outputId": "64053343-5575-404d-fbc2-b2e3fe4a4e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-12 13:08:22--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-09-12 13:08:22--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-09-12 13:08:22--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‚Äòglove.6B.zip‚Äô\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 38s  \n",
            "\n",
            "2025-09-12 13:11:01 (5.19 MB/s) - ‚Äòglove.6B.zip‚Äô saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_index = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "print(f\" Loaded {len(embedding_index):,} word vectors from GloVe\")\n",
        "embedding_dim = 100\n",
        "max_words = 10000  # Same as your tokenizer's num_words\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= max_words:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_t89ovSCxnf",
        "outputId": "a2bee090-4bc0-4e77-d09b-00d8938260c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 400,000 word vectors from GloVe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=200,\n",
        "              weights=[embedding_matrix], trainable=False),  # GloVe frozen\n",
        "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "n4MRc018Dvkg",
        "outputId": "168fc726-aff4-430b-dffd-bac7927fd41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'max_words' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1723413718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model = Sequential([\n\u001b[0;32m----> 7\u001b[0;31m     Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=200,\n\u001b[0m\u001b[1;32m      8\u001b[0m               weights=[embedding_matrix], trainable=False),  # GloVe frozen\n\u001b[1;32m      9\u001b[0m     \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required more time to run epochs and needed GPU Access"
      ],
      "metadata": {
        "id": "xrx68Qn7mDLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "accuracy_lstm= accuracy_score(y_test, y_pred)\n",
        "precision_lstm=precision_score(y_test, y_pred, average=\"weighted\")\n",
        "f1_lstm=f1_score(y_test, y_pred, average=\"weighted\")\n",
        "recall_lstm=recall_score(y_test, y_pred, average=\"weighted\")\n",
        "print(f\"Accuracy= {accuracy_lstm: .4f}\")\n",
        "print(f\"Precision= {precision_lstm: .4f}\")\n",
        "print(f\"F1 Score= {f1_lstm: .4f}\")\n",
        "print(f\"Recall= {recall_lstm: .4f}\")"
      ],
      "metadata": {
        "id": "27bGi-e_F3HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}